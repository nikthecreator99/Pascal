#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–£—Å—ã –ü–∞—Å–∫–∞–ª—è ‚Äî —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∫–∏–Ω–æ-–±–æ—Ç.

–§—É–Ω–∫—Ü–∏–∏:
- –ù–æ–≤–æ—Å—Ç–∏ –∫–∞–∂–¥—ã–µ 15 –º–∏–Ω—É—Ç: 15 –º–∏–Ω—É—Ç —Å–æ–±–∏—Ä–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (RSS + X/Nitter + IG-–∑–µ—Ä–∫–∞–ª–∞),
  –∑–∞—Ç–µ–º –ø—É–±–ª–∏–∫—É–µ–º –ª—É—á—à—É—é (–¥–ª—è –†–§-–∞—É–¥–∏—Ç–æ—Ä–∏–∏: —Ñ—Ä–∞–Ω—à–∏–∑—ã, —Ç–æ–ø-–∑–≤—ë–∑–¥—ã).
- –ü–æ—Å—Ç—ã ¬´–∫–∞—Ä—Ç–∏–Ω–∫–∞ —Å–≤–µ—Ä—Ö—É + –∫–æ—Ä–æ—Ç–∫–∏–π —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —Ç–µ–∫—Å—Ç¬ª (–±–µ–∑ —Å–ª–æ–≤–∞ ¬´–ó–∞–≥–æ–ª–æ–≤–æ–∫¬ª).
- 09:00 –∏ 21:00 ‚Äî –∞–∫—Ç—Ä–∏—Å–∞ –¥–Ω—è (—á–µ—Ä–µ–∑ IG-–∑–µ—Ä–∫–∞–ª–∞; –µ—Å—Ç—å —Ñ–æ–ª–±—ç–∫–∏).
- 18:00 ‚Äî 5 —Ñ–∏–ª—å–º–æ–≤ –Ω–∞ –≤–µ—á–µ—Ä (–ø—Ä–æ—Å—Ç–∞—è –ø–æ–¥–±–æ—Ä–∫–∞).
- 1‚Äì2 —Ñ–æ—Ç–æ —Å–æ —Å—ä—ë–º–æ–∫ –≤ –¥–µ–Ω—å (—á–µ—Ä–µ–∑ X/Nitter –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º).
- –†–∞–∑ –≤ 7 –¥–Ω–µ–π ‚Äî ¬´–í –∫–∏–Ω–æ –Ω–∞ —ç—Ç–æ–π –Ω–µ–¥–µ–ª–µ¬ª (TMDB now_playing, region=RU).
- –ï–∂–µ–¥–Ω–µ–≤–Ω–æ ‚Äî –¥–µ–Ω—å —Ä–æ–∂–¥–µ–Ω–∏—è –∏–∑–≤–µ—Å—Ç–Ω–æ–π –ø–µ—Ä—Å–æ–Ω—ã (Wikipedia + TMDB —Ñ–æ—Ç–æ).
- –ê–Ω—Ç–∏-–¥—É–±–ª–∏, –∫—ç—à–∏, —Ä–æ—Ç–∞—Ü–∏—è –∑–µ—Ä–∫–∞–ª, —Ç–∞–π–º–∞—É—Ç—ã, –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ —Ñ–æ—Ç–æ —Å –ø–µ—Ä–µ–∑–∞–ª–∏–≤–æ–º.
"""

from __future__ import annotations
import os, sys, re, io, json, time, random, logging, textwrap, html, mimetypes, hashlib
import datetime as dt
from typing import Optional, List, Dict, Tuple
from dataclasses import dataclass, field

import requests
try:
    import feedparser
except ImportError:
    os.system("pip install feedparser")
    import feedparser

# ---------- –±–∞–∑–æ–≤–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ ----------
BASE = os.path.dirname(os.path.abspath(__file__))
os.makedirs(os.path.join(BASE, "logs"), exist_ok=True)
os.makedirs(os.path.join(BASE, ".state"), exist_ok=True)

def load_env():
    # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π dotenv, –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    env = {}
    p = os.path.join(BASE, ".env")
    if os.path.isfile(p):
        for line in open(p, "r", encoding="utf-8"):
            line=line.strip()
            if not line or line.startswith("#"): continue
            if "=" in line:
                k,v=line.split("=",1)
                env[k.strip()]=v.strip()
    return env
ENV = load_env()
def E(key, default=""): return os.getenv(key, ENV.get(key, default))

TELEGRAM_BOT_TOKEN = E("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHANNEL_ID = E("TELEGRAM_CHANNEL_ID")
OPENAI_API_KEY = E("OPENAI_API_KEY","")
TMDB_API_KEY = E("TMDB_API_KEY","")
TZ = E("TZ","Europe/Amsterdam")
NEWS_INTERVAL_MIN = int(E("NEWS_INTERVAL_MIN","15"))
COLLECT_WINDOW_MIN = int(E("COLLECT_WINDOW_MIN","15"))

os.environ["TZ"]=TZ
try:
    time.tzset()
except Exception:
    pass

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(os.path.join(BASE,"logs","run.log"), encoding="utf-8"),
        logging.StreamHandler(sys.stdout)
    ]
)
err = logging.getLogger("err")
err.addHandler(logging.FileHandler(os.path.join(BASE,"logs","error.log"), encoding="utf-8"))

UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
      "(KHTML, like Gecko) Chrome/124.0 Safari/537.36")
S = requests.Session()
S.headers.update({"User-Agent": UA, "Accept-Language":"ru,en;q=0.9"})

# ---------- –∞–Ω—Ç–∏-–¥—É–±–ª–∏ ----------
SEEN_PATH = os.path.join(BASE, ".state", "seen.json")
def load_seen()->Dict[str,float]:
    if os.path.isfile(SEEN_PATH):
        try: return json.load(open(SEEN_PATH,"r",encoding="utf-8"))
        except Exception: return {}
    return {}
def save_seen(d:Dict[str,float]):
    try: json.dump(d, open(SEEN_PATH,"w",encoding="utf-8"))
    except Exception as e: err.error(f"save_seen: {e}")
SEEN = load_seen()

def h(s:str)->str: return hashlib.sha1(s.encode("utf-8","ignore")).hexdigest()
def now()->dt.datetime: return dt.datetime.now()

# ---------- Telegram ----------
def tg_api(method:str, data=None, files=None)->bool:
    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHANNEL_ID:
        err.error("TELEGRAM_* –Ω–µ –∑–∞–¥–∞–Ω—ã")
        return False
    url=f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/{method}"
    try:
        r=S.post(url, data=data, files=files, timeout=25)
        ok=r.ok and r.json().get("ok",False)
        if not ok:
            err.error(f"TG {method} failed: {r.text[:400]}")
        return ok
    except Exception as e:
        err.error(f"TG {method} ex: {e}")
        return False

def tg_send_text(text:str)->bool:
    return tg_api("sendMessage",{
        "chat_id": TELEGRAM_CHANNEL_ID,
        "text": text,
        "parse_mode":"HTML",
        "disable_web_page_preview": True
    })

def tg_send_photo(photo_url:str, caption:str)->bool:
    """–°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º URL; –µ—Å–ª–∏ TG –Ω–µ –º–æ–∂–µ—Ç —Å–∫–∞—á–∞—Ç—å ‚Äî –ø–µ—Ä–µ–∑–∞–ª–∏–≤–∞–µ–º –∫–∞–∫ —Ñ–∞–π–ª."""
    # 1) –∫–∞–∫ URL
    ok = tg_api("sendPhoto",{
        "chat_id": TELEGRAM_CHANNEL_ID,
        "photo": photo_url,
        "caption": caption,
        "parse_mode":"HTML"
    })
    if ok: return True
    # 2) –∫–∞—á–∞–µ–º –∏ –∑–∞–ª–∏–≤–∞–µ–º
    try:
        r=S.get(photo_url, timeout=30)
        r.raise_for_status()
        mime = r.headers.get("content-type") or mimetypes.guess_type(photo_url)[0] or "image/jpeg"
        ext = mimetypes.guess_extension(mime) or ".jpg"
        files={"photo": (f"photo{ext}", io.BytesIO(r.content), mime)}
        return tg_api("sendPhoto",{
            "chat_id": TELEGRAM_CHANNEL_ID,
            "caption": caption,
            "parse_mode":"HTML"
        }, files=files)
    except Exception as e:
        err.error(f"photo reupload fail: {e}")
        return False

# ---------- —É—Ç–∏–ª–∏—Ç—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞ ----------
def clamp(s:str, n:int)->str:
    s=re.sub(r"\s+"," ", s or "").strip()
    return s if len(s)<=n else s[:n-1]+"‚Ä¶"

def host(u:str)->str:
    try:
        return re.sub(r"^www\.", "", re.sub(r"\W","_", requests.utils.urlparse(u).hostname or "src"))
    except Exception:
        return "src"

def fetch(url:str, **kw)->Optional[requests.Response]:
    try:
        r=S.get(url, timeout=15, **kw)
        r.raise_for_status()
        return r
    except Exception as e:
        err.info(f"fetch fail {url}: {e}")
        return None

def extract_image(html_text:str)->Optional[str]:
    for pat in [
        r'property=["\']og:image["\'][^>]+content=["\']([^"\']+)["\']',
        r'name=["\']twitter:image["\'][^>]+content=["\']([^"\']+)["\']',
        r'<img[^>]+src=["\']([^"\']+)["\']'
    ]:
        m=re.search(pat, html_text, re.I)
        if m:
            img=m.group(1)
            if img.startswith("//"): img="https:"+img
            return img
    return None

# ---------- –∏—Å—Ç–æ—á–Ω–∏–∫–∏ ----------
NITTER_MIRRORS = [
    "https://nitter.net",
    "https://nitter.poast.org",
    "https://nitter.fdn.fr",
    "https://nitter.lacontrevoie.fr",
]
# –∫–ª—é—á–µ–≤—ã–µ X-–∞–∫–∫–∞—É–Ω—Ç—ã (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä; –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä—è—Ç—å —Ñ–∞–π–ª–æ–º)
X_HANDLES = [
    "Variety","THR","DEADLINE","IndieWire","TheWrap","empiremagazine",
    "screenrant","collider","NextBestPicture","DiscussingFilm","CultureCrave",
    "RottenTomatoes","Metacritic","BFI","TheAcademy","BAFTA","Cannes",
    "sundanceorg","berlinale","TIFF_NET","Tribeca","SXSW","A24","NEONrated",
    "Blumhouse","FocusFeatures","searchlightpics","warnerbros","UniversalPics",
    "SonyPictures","ParamountPics","Lionsgate","MarvelStudios","starwars"
]
# —Å–æ–∑–¥–∞—ë–º RSS url –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–µ—Ä–∫–∞–ª–∞; –±–æ—Ç —Å–∞–º –ø–æ–ø—Ä–æ–±—É–µ—Ç —Ä–∞–±–æ—á–µ–µ
def nitter_rss_urls()->List[str]:
    urls=[]
    for h in X_HANDLES:
        for base in NITTER_MIRRORS:
            urls.append(f"{base}/{h}/rss")
    return urls

# IG-–∑–µ—Ä–∫–∞–ª–∞ –∏ r.jina.ai (–ø—Ä–æ–∫—Å–∏)
IG_MIRRORS = [
    "https://r.jina.ai/http://instagram.com/{u}/?__a=1&__d=dis",  # JSON-–ø—Ä–æ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ
    "https://r.jina.ai/http://www.instagram.com/{u}/",            # HTML -> –≤ –Ω—ë–º –µ—Å—Ç—å JSON
    "https://www.picnob.com/profile/{u}/",                        # –∑–µ—Ä–∫–∞–ª–∞ (–º–æ–≥—É—Ç –º–µ–Ω—è—Ç—å—Å—è)
    "https://imginn.com/u/{u}/",
    "https://dumpor.com/v/{u}",
]

ACTRESS_IG = [
    "sydney_sweeney","margotrobbieofficial","meganfox","jessicaalba",
    "gal_gadot","salmahayek","sofiavergara","alexandradaddario","emmastone",
    "zendaya","annehathaway","jlo","vanessa_hudgens","kyliejenner","monica.bellucci",
    "karen_gillan","natalieportman","oliviarodrigo","dovecameron","camimendes",
    "lili_reinhart","madelainepetsch","emilyblunt","dakotajohnson","emmawatson",
    "chloemoretz","sofiacarson","haleyluhoo","haileesteinfeld",
    "lilyjcollins","anya_taylorjoy","sabrinacarpenter","madelyncline","ella_purnell",
    "sophiataylorali","maya_hawke","emilycarey","milliebobbybrown","sadiesink_",
    "hunter_schafer","rachelzegler","isabelamerced","barbarapalvin",
    "kathrynnewton","maudeapatow",
]

# RSS (–ø–ª—é—Å Nitter)
RSS_LIST = [
    "https://variety.com/feed/",
    "https://www.hollywoodreporter.com/feed/",
    "https://deadline.com/feed/",
    "https://www.indiewire.com/feed/",
    "https://www.thewrap.com/feed/",
    "https://www.empireonline.com/feed/all/",
    "https://www.slashfilm.com/feed/",
    "https://www.ign.com/rss/feeds/movies.xml",
    "https://editorial.rottentomatoes.com/feed/",
]

# ---------- –º–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö ----------
@dataclass
class NewsItem:
    title: str
    summary: str
    link: str
    image: Optional[str]
    source: str
    ts: float = field(default_factory=lambda: time.time())
    likes: int = 0
    shares: int = 0
    score: float = 0.0

# ---------- —Å–±–æ—Ä –∏–∑ RSS ----------
def parse_rss(url:str)->List[NewsItem]:
    out=[]
    try:
        feed=feedparser.parse(url)
        for e in feed.entries:
            title=html.unescape(e.get("title","")).strip()
            summary=html.unescape(re.sub("<[^>]+>","", e.get("summary",""))).strip()
            link=e.get("link","")
            img=None
            # media
            for key in ("media_content","media_thumbnail"):
                arr=e.get(key)
                if arr and isinstance(arr,list):
                    u=arr[0].get("url")
                    if u: img=u; break
            if not img and link:
                r=fetch(link)
                if r:
                    img=extract_image(r.text)
            if title and link:
                out.append(NewsItem(title, summary, link, img, url))
    except Exception as e:
        err.info(f"parse_rss {url}: {e}")
    return out

# ---------- —Å–±–æ—Ä –∏–∑ X —á–µ—Ä–µ–∑ Nitter RSS ----------
ONSET_KEYS = ["on set","behind the scenes","—Å–æ —Å—ä–µ–º–æ–∫","—Å–æ —Å—ä—ë–º–æ–∫","bts"]
def parse_nitter_rss(url:str)->List[NewsItem]:
    out=[]
    r=fetch(url)
    if not r: return out
    feed=feedparser.parse(r.text)
    for e in feed.entries[:5]:
        title=html.unescape(e.get("title","")).strip()
        summary=html.unescape(re.sub("<[^>]+>","", e.get("summary",""))).strip()
        link=e.get("link","")
        img=None
        # —É Nitter –≤ summary —á–∞—Å—Ç–æ <img ... src=> ‚Äî –≤—ã—Ç–∞—â–∏–º
        m=re.search(r'<img[^>]+src=["\']([^"\']+)["\']', e.get("summary",""), re.I)
        if m:
            src=m.group(1)
            if src.startswith("//"): src="https:"+src
            img=src
        if title and link:
            out.append(NewsItem(title, summary, link, img, url))
    return out

# ---------- IG mirrors: –ø–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –∫–∞–¥—Ä –ø—Ä–æ—Ñ–∏–ª—è ----------
def ig_latest_image(username:str)->Tuple[Optional[str], Optional[str]]:
    """–ü—Ä–æ–±—É–µ–º —Ä—è–¥ –∑–µ—Ä–∫–∞–ª/–ø—Ä–æ–∫—Å–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º (img_url, caption) –∏–ª–∏ (None,None)."""
    u=username
    for pat in IG_MIRRORS:
        url=pat.format(u=u)
        r=fetch(url)
        if not r: 
            continue
        t=r.text
        # 1) r.jina JSON: –∏—â–µ–º "display_url" –∏ –ø–æ–¥–ø–∏—Å—å
        if "jina.ai" in url:
            m=re.search(r'"display_url"\s*:\s*"([^"]+)"', t)
            if m:
                img=m.group(1).encode("utf-8").decode("unicode_escape")
                cap=None
                cm=re.search(r'"edge_media_to_caption"[^}]+?"text"\s*:\s*"([^"]*)"', t)
                if cm:
                    cap=cm.group(1).encode("utf-8").decode("unicode_escape")
                return img,cap
        # 2) HTML –∑–µ—Ä–∫–∞–ª: –æ–±—ã—á–Ω—ã–π og:image
        img=extract_image(t)
        if img:
            # –ø–æ–¥–ø–∏—Å—å –≤—ã—Ç–∞—â–∏–º –∫–∞–∫ –∑–∞–≥–ª—É—à–∫—É –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É/–æ–ø–∏—Å–∞–Ω–∏—é
            cap=None
            m=re.search(r'<meta[^>]+property=["\']og:description["\'][^>]+content=["\']([^"\']+)["\']', t, re.I)
            if m: cap = html.unescape(m.group(1))
            return img, cap
    return None, None

# ---------- —Å–∫–æ—Ä–∏–Ω–≥ –¥–ª—è –†–§-–∞—É–¥–∏—Ç–æ—Ä–∏–∏ ----------
RU_BOOST = [
    "–î—é–Ω–∞","Dune","Avatar","–ê–≤–∞—Ç–∞—Ä","Marvel","DC","Star Wars","–ó–≤—ë–∑–¥–Ω—ã–µ –≤–æ–π–Ω—ã",
    "–í–ª–∞—Å—Ç–µ–ª–∏–Ω –∫–æ–ª–µ—Ü","Lord of the Rings","–ì–∞—Ä—Ä–∏ –ü–æ—Ç—Ç–µ—Ä","Harry Potter",
    "–§–æ—Ä—Å–∞–∂","Fast & Furious","Mission: Impossible","Top Gun","Alien","Blade Runner",
    "Last of Us","–í–µ–¥—å–º–∞–∫","Witcher","Sonic","–°–æ–Ω–∏–∫",
    "–¢–æ–º –ö—Ä—É–∑","Tom Cruise","–ö–∏–∞–Ω—É –†–∏–≤–∑","Keanu Reeves","–†–∞–π–∞–Ω –ì–æ—Å–ª–∏–Ω–≥","Ryan Gosling",
    "–ú–∞—Ä–≥–æ –†–æ–±–±–∏","Margot Robbie","–ó–µ–Ω–¥–µ—è","Zendaya","–¢–∏–º–æ—Ç–∏ –®–∞–ª–∞–º–µ","Timoth√©e Chalamet",
    "–ü–µ–¥—Ä–æ –ü–∞—Å–∫–∞–ª—å","Pedro Pascal","–ö–∏–ª–ª–∏–∞–Ω –ú—ë—Ä—Ñ–∏","Cillian Murphy",
    "–†–æ–±–µ—Ä—Ç –î–∞—É–Ω–∏","Robert Downey","–°–∫–∞—Ä–ª–µ—Ç—Ç –ô–æ—Ö–∞–Ω—Å—Å–æ–Ω","Scarlett Johansson",
    "–§–ª–æ—Ä–µ–Ω—Å –ü—å—é","Florence Pugh","–ì–µ–Ω—Ä–∏ –ö–∞–≤–∏–ª–ª","Henry Cavill"
]
BRANDS = ["netflix","hbo","disney","warner","paramount","sony","marvel","dc","pixar","a24","universal"]

def brand_score(s:str)->int:
    s2=s.lower()
    sc=0
    for kw in RU_BOOST:
        if kw.lower() in s2: sc+=10
    for b in BRANDS:
        if b in s2: sc+=6
    return sc

def interest_score(it:NewsItem)->float:
    base = it.likes*0.6 + it.shares*0.8
    base += brand_score(it.title+" "+it.summary)
    # —Å–≤–µ–∂–µ—Å—Ç—å
    age = (time.time()-it.ts)/60
    if age <= 60: base += 5
    # –¥–ª–∏–Ω—É –ø–æ–¥—Ä–µ–∑–∞–µ–º
    if len(it.summary)>800: base -= 5
    return base

# ---------- —Ñ–æ—Ä–º–∞—Ç –ø–æ—Å—Ç–∞ ----------
def humanize(it:NewsItem)->Tuple[str, Optional[str]]:
    title=clamp(it.title, 140)
    body=re.sub(r"\s+"," ", (it.summary or "").strip())
    body=clamp(body, 480)
    body=textwrap.fill(body, width=90)
    caption=f"<b>{html.escape(title)}</b>\n\n{html.escape(body)}"
    if it.link:
        caption += f"\n\n<a href=\"{it.link}\">–ò—Å—Ç–æ—á–Ω–∏–∫</a>"
    return caption, it.image

# ---------- –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ ----------
def publish_best(cands:List[NewsItem])->bool:
    if not cands: 
        logging.info("–ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–µ—Ç")
        return False
    for it in cands:
        it.score = interest_score(it)
    cands.sort(key=lambda x: x.score, reverse=True)
    best=cands[0]
    key=h(best.link or best.title)
    if SEEN.get(key): 
        logging.info("–£–∂–µ –ø—É–±–ª–∏–∫–æ–≤–∞–ª–∏: –ø—Ä–æ–ø—É—Å–∫")
        return False
    caption,img=humanize(best)
    ok = tg_send_photo(img, caption) if img else tg_send_text(caption)
    if ok:
        SEEN[key]=time.time()
        save_seen(SEEN)
        logging.info(f"–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {best.title} (score={best.score:.1f})")
    return ok

# ---------- —Å–±–æ—Ä –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤ —Ç–µ—á–µ–Ω–∏–µ –æ–∫–Ω–∞ ----------
def collect_window(minutes:int)->List[NewsItem]:
    start=time.time()
    collected=[]
    rss_all = RSS_LIST[:] + nitter_rss_urls()  # –æ–±—ã—á–Ω—ã–µ RSS + X/Nitter RSS
    random.shuffle(rss_all)
    while (time.time()-start) < minutes*60:
        try:
            # RSS
            for u in rss_all[:15]:
                items = (parse_nitter_rss(u) if "/rss" in u else parse_rss(u))[:5]
                for it in items:
                    it.ts=time.time()
                    collected.append(it)
            time.sleep(20)
        except Exception as e:
            err.error(f"collect loop: {e}")
            time.sleep(5)
    # —É–Ω–∏–∫–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ —Å—Å—ã–ª–∫–µ
    uniq={}
    for it in collected:
        uniq[it.link]=it
    return list(uniq.values())

# ---------- —Ä—É–±—Ä–∏–∫–∏ ----------
def post_evening_movies():
    picks = random.sample([
        ("–ü–æ–±–µ–≥ –∏–∑ –®–æ—É—à–µ–Ω–∫–∞","–î—Ä–∞–º–∞ –æ –Ω–∞–¥–µ–∂–¥–µ –∏ –¥—Ä—É–∂–±–µ."),
        ("–ò–Ω—Ç–µ—Ä—Å—Ç–µ–ª–ª–∞—Ä","–ö–æ—Å–º–∏—á–µ—Å–∫–∞—è –æ–¥–∏—Å—Å–µ—è –æ –≤—Ä–µ–º–µ–Ω–∏ –∏ —Å–µ–º—å–µ."),
        ("–ë–µ–∑—É–º–Ω—ã–π –ú–∞–∫—Å: –î–æ—Ä–æ–≥–∞ —è—Ä–æ—Å—Ç–∏","–ü–æ—Å—Ç–∞–ø–æ–∫–∞–ª–∏–ø—Ç–∏—á–µ—Å–∫–∏–π —ç–∫—à–µ–Ω."),
        ("–û—Å—Ç—Ä–æ–≤ –ø—Ä–æ–∫–ª—è—Ç—ã—Ö","–ú—Ä–∞—á–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–∏–≤ —Å –ø–æ–≤–æ—Ä–æ—Ç–∞–º–∏."),
        ("–ü–∞—Ä–∞–∑–∏—Ç—ã","–°–æ—Ü–∏–∞–ª—å–Ω–∞—è —Å–∞—Ç–∏—Ä–∞-—Ç—Ä–∏–ª–ª–µ—Ä."),
        ("–î—é–Ω–∞","–≠–ø–∏—á–µ—Å–∫–∞—è –ù–§-—Å–∞–≥–∞ –ø–æ –ì–µ—Ä–±–µ—Ä—Ç—É."),
        ("–î–∂–æ–Ω –£–∏–∫","–°—Ç–∏–ª—å–Ω—ã–π –±–æ–µ–≤–∏–∫ –æ –º–µ—Å—Ç–∏."),
        ("–¢—Ä–∏ –±–∏–ª–±–æ—Ä–¥–∞‚Ä¶","–ñ—ë—Å—Ç–∫–∞—è –¥—Ä–∞–º–∞ —Å —á—ë—Ä–Ω—ã–º —é–º–æ—Ä–æ–º."),
        ("–ö–æ–∫–æ","–¢—ë–ø–ª–∞—è –∞–Ω–∏–º–∞—Ü–∏—è –æ —Å–µ–º—å–µ."),
        ("–¢–µ–º–Ω—ã–π —Ä—ã—Ü–∞—Ä—å","–ë—ç—Ç–º–µ–Ω –ø—Ä–æ—Ç–∏–≤ –î–∂–æ–∫–µ—Ä–∞."),
    ], 5)
    lines=["<b>5 —Ñ–∏–ª—å–º–æ–≤ –Ω–∞ –≤–µ—á–µ—Ä</b>"]
    for t,d in picks:
        lines.append(f"‚Ä¢ <b>{html.escape(t)}</b> ‚Äî {html.escape(d)}")
    tg_send_text("\n".join(lines))

def post_weekly_ru_cinemas()->bool:
    if not TMDB_API_KEY:
        logging.info("TMDB_API_KEY –Ω–µ—Ç ‚Äî –ø—Ä–æ–ø—É—Å–∫ weekly")
        return False
    r=fetch("https://api.themoviedb.org/3/movie/now_playing",
            params={"api_key":TMDB_API_KEY,"region":"RU","language":"ru-RU","page":1})
    if not r: return False
    arr=r.json().get("results",[])[:20]
    if not arr: return False
    lines=["<b>–í –∫–∏–Ω–æ –Ω–∞ —ç—Ç–æ–π –Ω–µ–¥–µ–ª–µ</b>"]
    for m in arr:
        title=m.get("title") or m.get("name") or m.get("original_title") or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
        date=m.get("release_date") or ""
        vote=m.get("vote_average") or 0
        lines.append(f"‚Ä¢ {html.escape(title)} ‚Äî {date} ‚Äî —Ä–µ–π—Ç–∏–Ω–≥ {vote:.1f}")
    return tg_send_text("\n".join(lines))

def post_birthday()->bool:
    # –±–µ—Ä—ë–º –∞–Ω–≥–ª. –∫–∞—Ç–µ–≥–æ—Ä–∏—é (—Å—Ç–∞–±–∏–ª—å–Ω–∞—è), –Ω–∞—Ö–æ–¥–∏–º –ø–µ—Ä—Å–æ–Ω—É-–∞–∫—Ç—ë—Ä–∞
    today=now().strftime("%B %d") # e.g. September 18
    r=fetch("https://en.wikipedia.org/w/api.php", params={
        "action":"query","list":"categorymembers",
        "cmtitle":f"Category:Births on {now().strftime('%B %d')}",
        "cmlimit":"50","format":"json"
    })
    if not r: return False
    names=[m["title"] for m in r.json().get("query",{}).get("categorymembers",[])]
    if not names: return False
    pri=["actor","actress","film","oscar","hollywood"]
    ranked=[]
    for n in names:
        s=n.lower()
        sc=sum(1 for p in pri if p in s)
        if sc: ranked.append((sc,n))
    if not ranked: return False
    ranked.sort(reverse=True)
    top=ranked[0][1]
    img=None
    if TMDB_API_KEY:
        sr=fetch("https://api.themoviedb.org/3/search/person",
                 params={"api_key":TMDB_API_KEY,"query":top,"language":"en-US"})
        if sr and sr.json().get("results"):
            p=sr.json()["results"][0].get("profile_path")
            if p: img=f"https://image.tmdb.org/t/p/w780{p}"
    text=f"<b>–°–µ–≥–æ–¥–Ω—è –¥–µ–Ω—å —Ä–æ–∂–¥–µ–Ω–∏—è:</b> {html.escape(top)} üéâ"
    return tg_send_photo(img, text) if img else tg_send_text(text)

def post_on_set()->bool:
    """–∏—â–µ–º —á–µ—Ä–µ–∑ Nitter-–ø–æ—Ç–æ–∫–∏ —Ç–≤–∏—Ç—ã —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ on set/bts/—Å–æ —Å—ä—ë–º–æ–∫."""
    urls=nitter_rss_urls()
    random.shuffle(urls)
    for u in urls[:12]:
        items=parse_nitter_rss(u)
        for it in items:
            s=(it.title+" "+it.summary).lower()
            if any(k in s for k in ONSET_KEYS):
                caption, img = humanize(it)
                if tg_send_photo(img, caption) if img else tg_send_text(caption):
                    return True
    return False

def post_actress(slot:str)->bool:
    uname=random.choice(ACTRESS_IG)
    img,cap=ig_latest_image(uname)
    title=f"{uname.replace('_',' ').title()} ‚Äî —Å–≤–µ–∂–∏–π –∫–∞–¥—Ä"
    body = cap or "–ö–∞–¥—Ä –¥–Ω—è –∏–∑ Instagram."
    body = clamp(body, 260)
    caption=f"<b>{html.escape(title)}</b>\n\n{html.escape(body)}\n\nInstagram: @{uname}"
    if img: return tg_send_photo(img, caption)
    else:   return tg_send_text(caption)

# ---------- —Ü–∏–∫–ª/—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ ----------
def should(hour:int, minute:int)->bool:
    n=now()
    return n.hour==hour and n.minute<minute

def run_news_once():
    logging.info(f"–°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π {COLLECT_WINDOW_MIN} –º–∏–Ω‚Ä¶")
    cands=collect_window(COLLECT_WINDOW_MIN)
    logging.info(f"–ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(cands)}")
    publish_best(cands)

def main():
    logging.info(f"–°—Ç–∞—Ä—Ç. TZ={TZ}. –ò–Ω—Ç–µ—Ä–≤–∞–ª={NEWS_INTERVAL_MIN} min, –æ–∫–Ω–æ —Å–±–æ—Ä–∞={COLLECT_WINDOW_MIN} min")
    last_news=0.0
    while True:
        try:
            n=now()
            daykey=n.strftime("%Y%m%d")

            def once(tag:str, cond:bool, fn):
                key=f"{tag}:{daykey}"
                if cond and not SEEN.get(key):
                    ok=False
                    try: ok=fn()
                    except Exception as e: err.error(f"{tag}: {e}")
                    SEEN[key]=time.time()
                    save_seen(SEEN)
                    logging.info(f"{tag}: {'ok' if ok else 'skip'}")

            # —Å–ø–µ—Ü-—Ä—É–±—Ä–∏–∫–∏
            once("weekly", (n.weekday()==0 and should(12,2)), post_weekly_ru_cinemas)  # –ø–Ω 12:00
            once("birthday", should(11,2), post_birthday)                               # 11:00
            once("onset14", should(14,2), post_on_set)                                  # 14:00
            once("onset19", should(19,2), post_on_set)                                  # 19:00
            once("actress_morning", should(9,2), lambda: post_actress("morning"))       # 09:00
            once("actress_evening", should(21,2), lambda: post_actress("evening"))      # 21:00
            once("evening_movies", should(18,2), post_evening_movies)                   # 18:00

            # –Ω–æ–≤–æ—Å—Ç–∏ –∫–∞–∂–¥—ã–µ NEWS_INTERVAL_MIN
            if time.time()-last_news >= NEWS_INTERVAL_MIN*60:
                run_news_once()
                last_news=time.time()

        except KeyboardInterrupt:
            break
        except Exception as e:
            err.error(f"loop: {e}")
        time.sleep(20)

# ---------- –±—ã—Å—Ç—Ä—ã–µ —Ç–µ—Å—Ç—ã ----------
def test_news():
    c=collect_window(1)
    publish_best(c)

def test_actress():
    post_actress("morning")

def test_weekly():
    post_weekly_ru_cinemas()

def test_birthday():
    post_birthday()

def test_onset():
    post_on_set()

if __name__=="__main__":
    cmd = (sys.argv[1] if len(sys.argv)>1 else "").lower()
    if cmd=="once": run_news_once()
    elif cmd=="test_news": test_news()
    elif cmd=="test_actress": test_actress()
    elif cmd=="test_weekly": test_weekly()
    elif cmd=="test_birthday": test_birthday()
    elif cmd=="test_onset": test_onset()
    else: main()
