"""
–¢—Ä–µ–Ω–¥—ã –¥–Ω—è: —á–∏—Ç–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ RSS-–ª–µ–Ω—Ç (Reddit / –∫–∏–Ω–æ), –≤—ã–±–∏—Ä–∞–µ–º 3‚Äì5 —Å–∞–º—ã—Ö –≥–æ—Ä—è—á–∏—Ö —Ç–µ–º –∏ –ø—É–±–ª–∏–∫—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–∞–π–¥–∂–µ—Å—Ç.
"""
import time, logging, feedparser, os
from common import send_telegram, build_caption, gpt_summarize

TREND_SOURCES = [
    "https://www.reddit.com/r/movies/.rss",
    "https://www.reddit.com/r/boxoffice/.rss",
    "https://www.reddit.com/r/horror/.rss"
]

MAX_ITEMS = int(os.getenv("TRENDS_MAX_ITEMS", "12"))
TOP_N = int(os.getenv("TRENDS_TOP_N", "5"))

def collect_trends():
    items = []
    for src in TREND_SOURCES:
        feed = feedparser.parse(src)
        for e in feed.entries[:MAX_ITEMS//len(TREND_SOURCES)+1]:
            title = getattr(e, "title", "")
            link = getattr(e, "link", "")
            if title and link:
                items.append(f"- {title}")
    return items[:MAX_ITEMS]

def build_post_text(items):
    joined = "\n".join(items[:TOP_N])
    prompt = ("–°–≤–µ–¥–∏ —Å–ø–∏—Å–æ–∫ –æ–±—Å—É–∂–¥–∞–µ–º—ã—Ö —Ç–µ–º –≤ –∫—Ä–∞—Ç–∫–∏–π –¥–∞–π–¥–∂–µ—Å—Ç –æ –∫–∏–Ω–æ. "
              "–°–¥–µ–ª–∞–π 3‚Äì5 –ø—É–Ω–∫—Ç–æ–≤, –∫–∞–∂–¥—ã–π ‚Äî 1 –∫–æ—Ä–æ—Ç–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ. "
              "–ë–µ–∑ –ª–∏—à–Ω–µ–π –≤–æ–¥—ã, –±–µ–∑ —Å–ø–æ–π–ª–µ—Ä–æ–≤.")
    summary = gpt_summarize(prompt + "\n–¢–µ–º—ã:\n" + joined, max_tokens=320)
    return summary or ("–¢—Ä–µ–Ω–¥—ã –¥–Ω—è:\n" + joined)

def main():
    items = collect_trends()
    if not items:
        logging.info("–¢—Ä–µ–Ω–¥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        return
    text = build_post_text(items)
    caption = build_caption("üî• –¢—Ä–µ–Ω–¥—ã –¥–Ω—è", text)
    ok = send_telegram(caption)
    if ok:
        logging.info("–¢—Ä–µ–Ω–¥—ã –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω—ã")
    else:
        logging.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ç—Ä–µ–Ω–¥—ã")

if __name__ == "__main__":
    main()
